\chapter{The Median Tree Problem}
\label{chap:mediantree}
Having surveyed the state of research on the \gls{pcstp} closely along with a
short summary of related problems. We now turn our eyes to the lattermost problem we have inspected:
\gls{mtp}.

As we noted in Chapter \ref{chap:related}, the problem involving the finding of \textit{median} trees
in graphs is not very well researched. To the best of our knowledge, no work has been put into solving
the problem in the general case to optimality. Being that this problem has some striking similarities
to the \gls{pcstp} -- that is, both problems involve finding the tree in a graph which minimises the sum
of the tree's edge costs and a penalty for every not-included vetex -- we suspect that refitting the
methods used to solve the \gls{pcstp} to the \gls{mtp} may result in good performance.

In this chapter, we will present a formal definition of the \acrlong{mtp} in Graphs. Then,
we will present a branch and cut algorithm and a solver for the \gls{mtp} which is inspired by the
work done on the \gls{pcstp}. Finally, we will present a dataset for the \gls{mtp} generated from a subset
of the DIMACS Steiner Tree challenge datasets for the \gls{pcstp}\citep{DIMACS},
and a set of experiments performed
on these.
 
\section{Problem Definition}

Let $G = (V, E, c, d)$ be an undirected graph. Denote $c : E \to \RR^+$ as an \textit{edge cost} function
and $d : V \times V  \to \RR^+$ be an \textit{assignment cost} function where we have
$$d_{ii} = 0 \mathnormal{.}$$
Then the \textit{\acrlong{mtp}}
is defined as finding a \textit{connected subgraph} $T = (V_T, E_T)$ of $G$
where $V_T \subseteq V$ and
$E_T \subseteq E$ which minimises the cost function,
$$c(T) = \sum_{ij \in E_T} c_{ij} + \sum_{i \in V} \min_{j \in V_T} d_{ij}\mathnormal{.}$$
We say that such a subgraph is a \textit{Median Tree} of $G$.

\begin{figure}[h]\centering
  \begin{subfigure}[b]{0.47\linewidth}\centering
    \begin{tikzpicture}[auto, node distance=1.5 cm]
      % Nodes
      \node[terminal] (a) {a};
      \node[terminal] (b) [right=of a] {b};
      \node[terminal] (c) [right=of b] {c};
      \node[terminal] (d) [above =of c] {d};
      \node[terminal] (e) [left=of d] {e};
      \node[terminal] (f) [left=of e] {f};
      \node[terminal] (g) [above right=0.75 and 1.3 of c] {g};
      % Edges
      \begin{scope}[every edge/.style={draw=black, thick}]
        \draw (a) edge node[below]{4} (b);
        \draw (b) edge node[near start]{5} (d);
        \draw (b) edge node[below]{8} (c);
        \draw (c) edge node{3} (d);
        \draw (c) edge node{2} (g);
        \draw (c) edge node[near start]{5} (e);
        \draw (d) edge node{6} (e);
        \draw (d) edge node{10} (g);
        \draw (e) edge node{1} (f);
      \end{scope}
    \end{tikzpicture}
    \caption{The graph $G$ with edge costs.}\label{fig:mtp:01:g}
  \end{subfigure}
  \begin{subfigure}[b]{0.47\linewidth}\centering
    \footnotesize
    \begin{tabular}{r||c|c|c|c|c|c|c}
 $d_{ij}$ & $a$ & $b$ & $c$ & $d$ & $e$ & $f$ & $g$ \\ \hline\hline
      $a$ &  0  &  5  &  8  &     &     &  6  &     \\ \hline
      $b$ &  2  &  0  &  2  &  2  &     &     &     \\ \hline
      $c$ &     &     &  0  & 10  &  4  &     &  1  \\ \hline
      $d$ &     &  3  &     &  0  &  6  &     &  2  \\ \hline
      $e$ &     &     &     &     &  0  &     &     \\ \hline
      $f$ &  2  &  8  &     &     &  5  &  0  &     \\ \hline
      $g$ &     &     &  5  &  5  &     &     &  0
    \end{tabular}
    \caption{The assignment cost function $d$.}\label{fig:mtp:01:d}
    \end{subfigure}
  \caption{Instance of the \gls{mtp} problem.}
  \label{fig:mtp:01}
\end{figure}

Figure~\ref{fig:mtp:01} shows an example of the \gls{mtp} represented as a graph
(\ref{fig:mtp:01:g}) taken from our recurring example
and an assignment function (\ref{fig:mtp:01:d}) which has been chosen with
no specfic purpose.
Figure~\ref{fig:mtp:01:opt}
shows the optimal solution
$$T = ( \{ c, e, f, g \}, \{(c, e), (c, g), (e, f)\})$$
to this problem with total cost
$$c(T) = (1 + 5 + 2) + (6 + 2 + 2) = 18\mathnormal{.}$$
\begin{figure}[h!]
  \centering
      \begin{tikzpicture}[auto, node distance=1.5 cm]
      % Nodes
      \node[terminal] (a) {a};
      \node[terminal] (b) [right=of a] {b};
      \node[terminal] (c) [right=of b] {c};
      \node[terminal] (d) [above =of c] {d};
      \node[terminal] (e) [left=of d] {e};
      \node[terminal] (f) [left=of e] {f};
      \node[terminal] (g) [above right=0.75 and 1.3 of c] {g};
      % Edges
      \begin{scope}[every edge/.style={draw=black, thick}]
        \draw (a) edge node[below]{4} (b);
        \draw (b) edge node[near start]{5} (d);
        \draw (b) edge node[below]{8} (c);
        \draw (c) edge node{3} (d);
        \draw (c) edge[selected] node{2} (g);
        \draw (c) edge[selected] node[near start]{5} (e);
        \draw (d) edge node{6} (e);
        \draw (d) edge node{10} (g);
        \draw (e) edge[selected] node{1} (f);
      \end{scope}
      \begin{scope}[every edge/.style={assignment}]
        \draw[->] (a) edge node{6} (f);
        \draw[->] (b) edge[bend right=60] node[below]{2} (c);
        \draw[->] (d) edge[bend left=60] node{2} (g);
      \end{scope}
    \end{tikzpicture}
    \caption{Optimal solution to the \gls{mtp} problem in Figure~(\ref{fig:mtp:01}).
      The edges of the facility are coloured in red and assignments are denoted with translucent red arrows.}\label{fig:mtp:01:opt}
  \end{figure}

  Since there exists a relative balance between the cost of the edges in the graph and the assignment cost, $T$
  only spans part of the graph. This raises a point. As assignment costs tend to infinity, the \gls{mtp} begins to look
  like the Minimum Spanning Tree problem -- this is akin to the \gls{stp} with $N = V$ and the \gls{pcstp} with infinite prize
  on all vertices. However, when edge costs tend toward infinity, the facility will naturally become a single vertex
  and the \gls{mtp} starts looking like the $1$-Median problem (See Section~\ref{sec:related:median}).
\todo[inline]{Maybe define terms Nonterminals and Supply points for vertices which
  respectively have either no assignment costs associated with them, are free to assign to any facility node,
  and are}

\paragraph{Reduction from the Prize-Collecting Steiner Tree Problem}
Instances of the \gls{pcstp} can straightforwardly be reduced to instances of the \gls{mtp}. This is in exact correspondence to how instances
of the Profitable Tour Problem can be reduced to instances of the Median Tour Problem (Section~\ref{sec:related:median}).
Given an instance of the \gls{pcstp} on graph $G = (V, E, c, p)$, define the assignment cost function

$$d_{ij} =
 \begin{cases}
   0 & i = j \\
   p_i & i \neq j
 \end{cases}\mathnormal{.}
 $$
 then solving the \gls{mtp} on the graph $G' = (V, E, c, d)$ is equivalent to solving the \gls{pcstp} on $G$. Since every vertex assignment not
 to the vertex itself pays the \textit{prize} of that vertex, every vertex not in the facility will pay its prize as penalty. Hence,
 any solution $T = (V, E, c, p)$ to the \gls{pcstp} on $G$ will have the exact same cost as the solution $T' = (V, E, c, d)$ to the \gls{mtp} on $G'$.

\begin{figure}[h]\centering
  \begin{subfigure}[b]{0.47\linewidth}\centering
    \begin{tikzpicture}[auto, node distance=1.5 cm]
      % Nodes
      \node[terminal] (a) {a};
      \node[terminal] (b) [right=of a] {b};
      \node[terminal] (c) [right=of b] {c};
      \node[terminal] (d) [above =of c] {d};
      \node[terminal] (e) [left=of d] {e};
      \node[terminal] (f) [left=of e] {f};
      \node[terminal] (g) [above right=0.75 and 1.3 of c] {g};
      % Edges
      \begin{scope}[every edge/.style={draw=black, thick}]
        \draw (a) edge node[below]{4} (b);
        \draw (b) edge node[near start]{5} (d);
        \draw (b) edge node[below]{8} (c);
        \draw (c) edge node{3} (d);
        \draw (c) edge node{2} (g);
        \draw (c) edge node[near start]{5} (e);
        \draw (d) edge node{6} (e);
        \draw (d) edge node{10} (g);
        \draw (e) edge node{1} (f);
      \end{scope}
    \end{tikzpicture}
    \caption{The graph $G$ with edge costs.}
    \label{fig:mtp:pcstp:g}
  \end{subfigure}
  \begin{subfigure}[b]{0.47\linewidth}\centering
    \footnotesize
    \begin{tabular}{r||c|c|c|c|c|c|c}
 $d_{ij}$ & $a$ & $b$ & $c$ & $d$ & $e$ & $f$ & $g$ \\ \hline\hline
      $a$ &  0  &  12 &  12 &  12 &  12 &  12 &  12 \\ \hline
      $b$ &  0  &  0  &  0  &  0  &  0  &  0  &  0  \\ \hline
      $c$ &  0  &  0  &  0  & 0  &  0  &  0  &  0  \\ \hline
      $d$ &  10 &  10 &  10 &  0  &  10 &  10 &  10 \\ \hline
      $e$ &  0  &  0  &  0  &  0  &  0  &  0  &  0  \\ \hline
      $f$ &  0  &   0 &  0  &  0  &  0  &  0  &  0  \\ \hline
      $g$ &  3  &   3 &  3  &  3  &  3  &  3  &  0
    \end{tabular}
    \caption{The assignment cost function $d$.}
  \end{subfigure}

  \begin{subfigure}[b]{0.60\linewidth}\centering
    \begin{tikzpicture}[auto, node distance=1.5 cm]
      % Nodes
      \node[terminal] (a) {a};
      \node[terminal] (b) [right=of a] {b};
      \node[terminal] (c) [right=of b] {c};
      \node[terminal] (d) [above =of c] {d};
      \node[terminal] (e) [left=of d] {e};
      \node[terminal] (f) [left=of e] {f};
      \node[terminal] (g) [above right=0.75 and 1.3 of c] {g};
      % Edges
      \begin{scope}[every edge/.style={draw=black, thick}]
        \draw (a) edge[selected] node[below]{4} (b);
        \draw (b) edge[selected] node[near start]{5} (d);
        \draw (b) edge node[below]{8} (c);
        \draw (c) edge node{3} (d);
        \draw (c) edge node{2} (g);
        \draw (c) edge node[near start]{5} (e);
        \draw (d) edge node{6} (e);
        \draw (d) edge node{10} (g);
        \draw (e) edge node{1} (f);
      \end{scope}
      \begin{scope}[every edge/.style={assignment}]
        \draw[->] (c) edge[bend left=60] node{0} (b);
        \draw[->] (e) edge node[left]{0} (b);
        \draw[->] (f) edge node{0} (a);
        \draw[->] (g) edge[bend right=60] node[above]{3} (d);
      \end{scope}
    \end{tikzpicture}
    \caption{Optimal Solution to (\ref{fig:mtp:pcstp:g}).}
  \end{subfigure}

  \caption{Instance of the \gls{pcstp} (Figure~\ref{fig:pcstp:01}) with optimal solution.}
  \label{fig:mtp:pcstp}
\end{figure}

Figure~\ref{fig:mtp:pcstp} shows how this reduction works on the \gls{pcstp} instance in Figure~\ref{fig:pcstp:01}
as well as the optimal solution
$$T = (\{a,b,d\}, \{(a,b), (b,d)\})$$
with cost
$$c(T) = (4 + 5) + (3) = 12$$
which is the same cost as the optimal solution to the original \gls{pcstp} problem (Figure~\ref{fig:pcstp:01:opt}).

While it is obvious that the \gls{mtp} problem is NP-hard -- something which has already been established in previous literature
 (See Section~\ref{sec:related:median}) -- this reduction again implies the NP-hardness of the problem.
 \section{Applications}
 We expect the \acrlong{mtp} to have similar applications to the \gls{pcstp} as they are fairly similar problems.

 Foremost, any kind of problem which involves a two tiered supply system could possibly be modelled as an instance of the
 \gls{mtp}. Take, for example, the placement of postal warehouses --- building and staffing a new warehouse comes at
 a cost but makes easier the delivery of packages to the surrounding area. The cost of warehouses (and the cost of maintaining
 a transport networks between them) could be modelled as edge costs, while the cost of delivering post from a warehouse to an area
 could be modelled as assignment costs.  Similar scenarios could exist in water supply, military supply lines, aid distribution etc.

 Another area of interest may be within computation biology, which as seen applications for the \gls{pcstp} and similar
 \citep{sun2018classical, akhmedov2016divide}.
\section{Integer Programming Formulation}

Given an instance of the \gls{mtp} as defined above,
Formulation~(\ref{form:mtp:cut}) is an \gls{ilp} formulation of the \gls{mtp}.
It is loosely inspired by the one defined by \citet{lucena2004strong}
(Formulation~\ref{form:lower:gsec}, Section~\ref{sec:lower:gsec}) for the Prize-Collecting
Steiner Tree Problem.

The variables $\bd x$ and $\bd y$ are boolean
decision vectors which are interpreted as follows.
As with the \gls{pcstp} formulation, when $x_{ij} = 1$,
the edge between vertices $v_i$ and $v_j$ is part of the solution -- note that only
one of $x_{ij}$ and $x_{ji}$ are variables in model and we will use both indices interchangably
to refer to the same variable for the sake of readability.

Similarly, $\bd y$ describes the assignment relation of a solution. When
$y_{ij} = 1$, vertex $v_i$ is \textit{assigned} to vertex $v_j$ and the corresponding
assignment cost must be paid.
These relations are reflected in the objective function.
When $y_{kk} = 1$, we consider that vertex assigned to
itself, which implies that vertex $v_k$ is part of the facility.

 \begin{formulation}[h!]
   \begin{subequations}
     \begin{alignat}{3}
       &\underset{\bd x, \bd y}{\text{minimize}}
       & & \sum_{ij \in E} c_{ij} x_{ij} +  \sum_{i, j \in V} d_{ij}y_{ij}  & \\
       & \text{subject to}\quad
       & & \sum_{ij \in E} x_{ij} = \sum_{i \in V} y_{ii} - 1 &&  \label{form:mtp:tree}\\
       &&& x(E(S)) \leq \sum_{i \in S \setminus \{s\}} y_{ii}
       && \forall S \subseteq V, s \in S \label{form:mtp:gsec} \\
       &&& \sum_{j \in V} y_{kj} = 1 && \forall k \in V \label{form:mtp:assignment}\\
       &&& y_{ik} \leq  y_{kk}
       && \forall i, k \in V \label{form:mtp:facility}\\
       &&& y_{kk} \leq \sum_{i \in \gls{delta}(k)} x_{ik}
       && \forall k \in V \label{form:mtp:legal} \\
       &&& \bd x \in \BB^{|E|} && \\
       &&& \bd y \in \BB^{|V \times V|}
     \end{alignat}\label{form:mtp:cut}
   \end{subequations}
   \caption{TBD}
 \end{formulation}

 Since edges have nonnegative weights, we know that the shape of the facility must be a tree.
 Constraint (\ref{form:mtp:tree}) ensures that any facility has exactly one less edge than
 vertices, and constraints (\ref{form:mtp:gsec}) -- commonly known as \glspl{gsec}
 -- ensures that no subset of vertices in the facility can form
 a cycle. Together, these ensure that any feasible
 solution must be a single, connected subgraph of $G$
 which is shaped as a tree.

 The rest of the constraints are concerned with making sure that every vertex in $G$ is
 assigned to a vertex in the facility in any feasible solution. Constraint
 (\ref{form:mtp:assignment}) ensures that every vertex in $G$ is assigned to exactly one
 vertex, and constraint (\ref{form:mtp:facility}) ensures that a vertex can only be assigned
 to if it is part of the facility. This could alternatively be modelled as
 $$\sum_{i \in V} y_{ik} \leq  M y_{kk}  \qquad \forall i \in V$$
 for some $M \gg |V|$.

 Finally, the constraints (\ref{form:mtp:legal}) ensure that no vertex can feasibly be part
 of the facility unless an edge in its adjacency is selected -- that is, unless it is spanned
 by the solution tree.

 \paragraph{Valid Inequalities}
 Whenever a vertex is connected to the facility, it trivially must be assigned to itself. This
 follows directly from the fact that $d_{ii} = 0$ for all $i \in V$ and thus have $d_{ij} \geq d_{ii}$
 for all $j \in V$. The fact that this is implied by the probe (i.e. it is always correct to assign a
 vertex in the facility to itself) allows us to not formalise it in the problem. However, we can
 still add the constraints
 \begin{equation}\label{form:mtp:str}
 y_{ii} \geq x_{ji} \qquad \forall i \in V,  \forall j \in \delta(i)
\end{equation}
 to as valid constraints to formalise this relation.

 Similar to how \citeauthor{ljubic2005solving} constrain the degree of Nonterminals in Constraint
 (\ref{form:exact:strength}), the same idea holds for the \gls{mtp} but in a slightly different manner.
 Consider the set of vertices which cost nothing to assign and cannot be assigned to as \textit{Nonterminals},
 $$N = \left\{ i \in V \middle\vert \sum_{j \in v} d_{ij} = 0 \wedge  \forall j. d_{ji} = \infty\right\}\mathnormal{.}$$
 As nonterminals play no part in the assignment question, they cannot be a leaf node in an optimal solution as they could trivially
 be removed from the facility to produce a solution with no greater cost. Thus they
 must have degree of at least
 two should they be part of an optimal facility, and it is valid to add the constraint
 \begin{equation}\label{mtp:valid:deg}
   \sum_{j \in \delta(i)}x_{ij} \geq 2 x_{ik} \qquad \forall i \in N, \: \forall k \in \delta(i)
 \end{equation}
 to the model. The intuitive interpretation of (\ref{mtp:valid:deg}) is that if any edge adjacent to
 a nonterminal is selected, \textit{at least} two edges adjacent to the nonterminal must be selected.
\section{Solver}
As mentioned previously, to test out the applicability of some of the methods introduced in
the Chapter \ref{chap:solving}, we have implemented a solver for the \gls{mtp}.
This section both goes over algorithmic and technical implemention details of the solver

The solver makes use of the general purpose \gls{mip} solver
Gurobi \citep{gurobi} (version 8.0.1)
and its Python 3 interface. While this means that the callbacks written for the solver
will naturally have much slower performance than if, for example, we had chosen to make
use of the C or C++ interface, the Python interface was chosen for its higher level interface
and better ease-of-use -- giving better iteration times. Having no other computational results
to compare against, this trade-off was easy to make.

The Python library,
NetworkX\footnote{\url{https://networkx.github.io/}} \citep{hagberg2008exploring}
was used for the internal graph representation of the
problem instances in the solver as well as for implementation of common
graph algorithms. It is worth noting that NetworkX features algorithm implementations
in native Python instead of the more efficient Python C modules. This, again, has some
performance implications especially with regards to user callbacks (which we will
 touch on below).

The source code of the solver as well as the benchmark suite
can be found on GitHub\footnote{\url{https://github.com/wsprent/thesis-code}}.

\paragraph{Branch and Cut}
The choice of applying a branch and cut method in the solver, is fairly straight
forward to make as it is infeasible to load all of the exponential number of
\glspl{gsec} into the model initially. Thus, the solver
\textit{must} make use of
Gurobi's \textit{Lazy Constraints} to avoid producing infeasible solutions
and \textit{may} additionally make use of \textit{User Cuts} to potentially
give faster runtimes by producing tighter lower bounds.

Both of these are included in the solver through Gurobi's callback system.
\subparagraph{Lazy Constraints}
The job of the lazy constraint callback involves separating integer solutions
which violate any \glspl{gsec}. This boils down to finding cycles in the solution
graph.

Given an integer solution $(\bd{\hat{x}}, \bd{\hat{y}})$ produced by Gurobi, we
identify any violated \glspl{gsec} by first generating a NetworkX graph,
$$S = (\{i \in V \mid \hat y_{ii} = 1 \}, \{ (i, j) \in E \mid \hat x_{ij} = 1\})\mathnormal{.}$$
Then we again use NetworkX to find all simple cycles in $S$. Then for each vertex
subsets $C \subseteq V$ induced by a cycle
in $S$, we add all of the violated constraints
$$x(E(C)) \leq y_{ii} \setminus \{c\} \qquad \forall c \in C$$
to the model.
\subparagraph{User Cuts}
The callback responsible for
Separating infeasible LP solutions with regards to \glspl{gsec}
have been adapted for the solver from the max-flow/min-cut  method used by \citet{lucena2004strong}
 (see Section~\ref{sec:solving:lower})
-- and later \citet{ljubic2005solving} and \citet{gamrath2017scip} -- for the \gls{pcstp}.

Adapting this separation procedure can be done directly. Having the decision variable
$\hat y_{ii} = b$ for some $b \in [0,1]$ in the LP relaxation of
the \gls{mtp} corresponds directly to $\hat y_i = b$ with respect to the \glspl{gsec}.
Once again, we use NetworkX to both represent the support graph and solve max flow problem(s).

As with the DHEA solver by \citet{ljubic2005solving}, we allow for controlling the maximum amount
of cuts performed at each node in the b\&b tree
with a \texttt{-{}-max-cuts} option. Additionally, this can be set
to \texttt{0} to disable the user cut routine altogether.

\paragraph{Primal Heuristics}
The solver an optional custom primal heuristic adapted from the primal hueristics detailed
for the \gls{pcstp} by \citet{ljubic2005solving}
(see Sections \ref{sec:solving:exact} and , \ref{sec:heuristics:lp}).

As with the cuts above, we exploit the fact that $y_i$ in the \gls{pcstp}
corresponds directly to $y_{ii}$ in the \gls{mtp}.

One thing we have not adapted, however, is the last step of solving the \gls{mtp}
on the final tree. This is left for further research.

The primal heuristic can be disabled in favour of Gurobi's inbuilt heuristic with
the switch $$\texttt{-{}-no-heuristics}\mathnormal{.}$$
\section{Computational Experience}
With having no prior attempts to solve the \gls{mtp} in the general case to draw from,
we have no interesting timings to test our model against nor any previously constructed
datasets to perform benchmarks on. Thus, we present in this section two things,
\begin{enumerate}
\item The specifics of how we have generated a new dataset for the \gls{mtp} out of previously released,
  as well as the details of the dataset, and
\item The result of performing benchmarks testing the effectiveness of
  \begin{enumerate*}[label={\alph*)}, font=\bfseries]
  \item our primal heuristic,
  \item the user cut routine, and
  \item the extra constraint (\ref{form:mtp:str}).
  \end{enumerate*}
\end{enumerate}
In short, we are looking to test out our additions to a what could be called a ``default'' Gurobi
implementation of the \gls{mtp}.

First we will give an overview of the dataset followed by an inspection of the three experiments we have
 performed.
 \subsection{Datasets}
 To perform realistic and relevant benchmarks of the settings in our solver, we need to run the solver
 against instances of the \gls{mtp} which are large enough to not be presolved and preferably small
 enough to solve to optimality within a reasonable timeframe. At the same time, as this is an introduction
 to the problem, we want the instances to be generic, opposed to modelling some specfic application or
 flavour of the \gls{mtp}.

 In getting to this point, our starting point is the \texttt{JMP} dataset presented by 
 \citet{Johnson:2000:PCS:338219.338637}. This was picked mainly for featuring the smallest
 and easiest to solve instances of the \gls{pcstp} instances available
 (and yet -- as shown below -- we have had to truncate the instances).
 The input instances are distributed in the \texttt{STP} format used by the \textit{SteinLib}
 library \citep{koch2001steinlib}.

 The \gls{mtp} instances we output are described in \texttt{STP} format with a small extension.
 This involves the addition of a new \texttt{Section} named \texttt{AssignmentCosts} each line
 in this section describes the cost of assigning a single vertex to another.

 Formally, each line has the format,
 \begin{grammar}
   <assignment> := `D' `\ ' <vertex-id> `\ ' <vertex-id> `\ ' <cost>
 \end{grammar}
 where \textit{<vertex-id>} is an integer id which corresponds to a vertex declared in the \texttt{Graph}
 section of the same file, and \textit{<cost>} is a non-negative real number.

 For example, the following example reads as \textit{the cost of assigning vertex ``1'' to vertex ``2'' is $4.5$.}
\begin{verbatim}
D 1 2 4.5
\end{verbatim}
Appendix \ref{app:report:stp} shows Figure~\ref{fig:mtp:01} represented in the extended STP format.
 \paragraph{Translation}
 In translating the \gls{pcstp} instances into \gls{mtp} instances, we keep all the of the graph
 structure including the edge costs. Hence we only need to generate a set of assignment costs which
  \textit{balanced} with respect to edge costs.

 To generate assignment costs, we first assign any nonterminal with no prize a prize
 equal to the average prize across all vertices. For each vertex, $i$, the cost to assign
 it to some vertex, $j$, is then,
 $$d_{ij} =
 \begin{cases}
   p_i \cdot \frac{sp(i, j)}{\sum_{l \in V}sp(i,l) / |V|} & p_i > 0 \\
      \frac{\sum_{l \in V} p_l}{|V|} \cdot \frac{sp(i, j)}{\sum_{l \in V}sp(i,l) / |V|} & p_i = 0
 \end{cases}\mathnormal{.}
$$
 In other words, vertex $i$'s prize is factored by ratio between length of the shortest path
 between $i$ and $j$ relative to the average distance between vertex $i$ and all vertices.
 This keeps the prizes in the original dataset relevant, and makes ``closeness'' between
 verties with respect to assignment costs somewhat transitive.

 A facet --- perhaps a weakness --- of this method is that it produces somewhat uniform
 assignment costs, and that it requires a connected graph to produce an \gls{mtp} instance
 with a lower-than-infinity optimal value. Furthermore, on connected graphs, all vertices
 are assignable to all vertices, and no vertex is assignable to another vertex at no cost.

 Some of the instances in the \texttt{JMP} dataset are disconnected. For these instances,
 before performing the above, we connect the graph by selecting a random pair of vertices,
 $i,j$, from disjoint components. The edge $(i,j)$ is then added to the edge set with cost
 $$c_{ij} = \sum_{k \in \delta(i)} c_{ik} / |\delta(i)|\mathnormal{.}$$

 The summarise, the output dataset features instances which have connected graphs with
 \textit{smooth} -- as opposed to erratic -- assignment cost functions. All vertices
 can be assigned to all other vertices without incurring infinite cost, and no vertex
 can be assigned to another vertex at zero cost. 

 \paragraph{Truncation} However, even though we chose the ``easiest'' of the available datasets,
 our solver still struggle with our translated instances, and would require a timeout
 value too large time constraint wise.

 Thus we have chosen to create smaller instances by truncating the translated \texttt{JMP}
 instances. The translated dataset mainly contains instances with either $100$ or $400$ vertices,
 and we truncate these into instances containing $60$ and $80$ vertices respectively.

 We accomplish this by iteratively contracting randomly selected edges. To avoid generating a
 vastly more connected graph, we then remove adjacent edges from the resulting vertex until
 we have $|\delta(i)| \frac{n_{target}}{|V|}$ edges in its adjacency (where $i$ is the vertex
 and $n_{target}$ is the number of vertices in the truncated instance) or until we randomly select
 an edge which would disconnect the graph.

 This method is admittedly rather arbitrary, but it aims to accomplish a graph containing similar
 connectivity as well as introducing a level of randomness to ---hopefully--- avoid a too uniform graph.

 The end result from the above is two sets of instances, \texttt{JMP-60} and \texttt{JMP-80}, named
 after the original dataset and the amount of vertices they contain.
 
 \subsection{Experiments}

 To perform our three experiments, we have developed a small rudimentary benchmarking suite. The solver
 itself is equipped with the ability to time itself and perform repeated solving of the same problem.
 For each problem instance, the solver is run with a \texttt{-{}-timeout} of five minutes. This timeout
 is passed directly to Gurobi, so the actual timeout is five minutes plus the time it takes to load the
 graph etc. Each instance is solved five times (the problem is loaded from file each of the five times)
 and the all times reported below is the mean these five timings.

 Executions are grouped by instance, that is for each instance, the solver is run with the following
 settings in order:
 \begin{enumerate}
 \item No user cuts (\texttt{-{}-max-cuts 0 -{}-no-heuristics}).
 \item Maximum of one cut per node (\texttt{-{}-max-cuts 1 -{}-no-heuristics}).
 \item Maximum of 25 cuts per node (\texttt{-{}-max-cuts 25 -{}-no-heuristics}).
 \item Default settings.
 \item With constraint~(\ref{mtp:valid:deg}) (\texttt{-{}-strengthen -{}-no-heuristics}).
 \end{enumerate}
 Grouping benchmarks by instances was done to hopefully improve the chances of each run happening
 under the same circumstances while the precise ordering of the settings is arbitrary.
 
 Default settings solves the input problem on the basic model (Formulation~\ref{form:mtp:cut})
 with a maximum of 25 user cuts, and performs the primal heuristics defined above at every
 25th node.

\textit{System info}

\paragraph{Heuristics}
In this experiment, we look at the performance of our primal heuristic, defined above, versus
the inbuilt heuristic from Gurobi. While the Gurobi heuristic, which is based on rounding
of the the LP solution, has the advantages of being written in a systems programming language
with less overhead and ---probably--- has hooks directly into the core optimiser,
implementing a strong heuristic for the \gls{mtp} should be able to out perform it.

In the case of this particular formulation of the \gls{mtp},
for example, the Gurobi optimiser does not have access to the
many \glspl{gsec}, making it prone to produce infeasible solutions which waste time.

However, our heuristic is rudimentary, adapted directly from a neighbouring problem,
and implemented in an interpreted language. It may struggle to keep up.

For our benchmarks, we have kept the default Gurobi setting of approximately running
the default heuristic $5\%$ of the time when enabled. Our heuristic instead runs on
every 25th node.

\begin{table}[h!]
  \centering
  \begin{tabular}[h!]{|c|c|c|c|c|}\hline
    \input{heuristics-table.tex}
  \end{tabular}
  \caption{Timing results for the primal heuristic benchmark. The
    \textbf{bolded} times are the fastest.}
  \label{tab:jmp:heuristics}
\end{table}

In Table~\ref{tab:jmp:heuristics} we show the timings of running the heuristics experiment.
In both cases, all instances were solved to optimality within the five minute time limit, which
is shown by the $\mathbf{0\%}$ $GAP$ between the highest lower bound and best incumbent.
The results are somewhat mixed, with the Gurobi heuristic proving to be faster in the majority
of cases. In a couple of cases, particularly instances \texttt{K400.1} and \texttt{K400.5},
our primal heuristic is twice as slow as the Gurobi heuristic. The picture of having the
custom primial heuristic only sometimes being worthwhile mirrors results from
\citet{ljubic2005solving}, who experienced the same ambivalent results for their solver.

While these results suggest that turning off the Python-based heuristic is the way to go
in most cases,
they also suggest that perhaps by improving the heuristic we can achieve performance
which eclipses the default performance.

How we should achieve this, depends heavily on which way the heuristic is ``underperforming''.
 Is it producing solutions of too high value? Is it running too slowly?

 If speed is a problem, a good first option to improve performance,
 would be to rewrite the heuristic procedure
in a systems programming language (e.g. C, C++, Rust).
This should by default give a multi-factor speedup, and gives better options for
later performance optimisation.

If the heuristic isn't producing good enough solutions, then we could look at the final step
of the algorithm. In our implementation we do not solve the \gls{mtp} on the final tree contrary
to the original algorithm defined by \citet{ljubic2005solving}. It may be that by skipping
this step, we are missing out on a lot.

In both cases, simply running the heuristic on every 25th node is not a very cunning
way of using our resources, and there is no reason to believe that this achieves optimal
performance. If the heuristic was reimplemented to be blazingly fast, it may be advisable to
simply run the full heuristic at every node of the branch and bound tree
(which mirrors what we have seen in Section~\ref{sec:solving:exact}). However, it may also
be a good idea to change behaviour based on the nature of the LP solution, either by
running a different heuristic when the LP solution far from integral or \textit{only}
running the heuristic when the solution is close to integral. 

Either way, our results seem to confirm that reusing the heuristic from
\citeauthor{ljubic2005solving} is not a faulty strategy, and that the \gls{pcstp}
and \gls{mtp} similar enough for knowledge sharing.

\paragraph{User Cuts}
A very different story is told by our user cuts experiment. Table~\ref{tab:jmp:maxcut}
displays the results of our user cut experiment. The results are very clear, the performance
of our user cut routine is atrocious compared to disabling it altogether. In most instances
we see more than a $10 \times$ slowdown for the MC-25 configuration compared to the MC-0
configuration, with the differential peaking for instance \texttt{K100.2} at around $30 \times$.

These results also shown the only example of our solver not being able to solve an instance
within the given time limit. Instance \texttt{K100.8} proves too difficult for the solver
given the default \texttt{-{}-max-cuts 25} setting, causing it to terminate on average with
a LP/incumbent gap of $6.52\%$.

Such stark differences between configurations
raise the question of whether or not we are talking about a performance
related bug in the implementation of the user cut routine.
While this may be the case, we will proceed as if it is not.

\begin{table}[h!]
  \centering
  \begin{tabular}[h!]{|c|c|c|c|c|c|}\hline
    \input{max-cut-table.tex}
  \end{tabular}
  \caption{Timing results for running the user cut experiments. The columns MC-0,
    MC-1, and MC-25 shown the timings for running the solver with no user cuts, max
    one user cut per node, and max 25 user cuts per node respectively.
    \textbf{Bold} font indicates fastest time; \textit{Italic} font indicates
    timeout; the percentage GAP is shown is the highest across the three configurations.}\label{tab:jmp:maxcut}
\end{table}

Having to potentially solve a max-flow problem for each vertex in the input graph incurs a runtime
of $O(n^2 \log n)$. This makes the separation procedure quite costly compared
to our primal heuristic.
Spending this much time in the Python interpreter may actually be enough to explain the performance
disrepancy.

Furthermore, enabling user cuts in Gurobi requires the disabling of the \texttt{PreCrush} setting
which disallows Gurobi from reformulating constraints when presolving the model. This means
that a user cut routine must not just justify itself in terms of time spent versus tigher bounds,
but also with respect to a weakening of the presolving routine.

In the end, these results show that disabling user cuts altogether in our model is definitely
the optimal option.

\paragraph{Strengthening the Model}
Our final experiment involves modifying the model itself instead of behaviour of the Gurobi solver.
This obviously involves a trade off between increasing the size of the model and reducing the size
of the feasible region.

\begin{table}[h!]
  \centering
  \begin{tabular}[h!]{|c|c|c|c|c|}\hline
    \input{strengthen-table.tex}
  \end{tabular}
  \caption{Timing results with and without constraint constraints~(\ref{form:mtp:str}).
    \textbf{Bold} font indicates fastest time;
    \textit{Italic} font indicates
    timeout; the percentage GAP is shown is the highest across the two configurations.} 
  \label{tab:jmp:strengthen}
\end{table}

Table~\ref{tab:jmp:strengthen} shows the effects of adding constraints~(\ref{form:mtp:str}) to
the model. For most of the instances there is very little difference between the two models
of the problem in how long it takes to reach an optimal solution. Notably, \texttt{K100.8} is
solved in $223$ seconds with the strengthened model yet times out on the basic model at $310$
seconds. The only other stark runtime difference of this caliber is on instance \texttt{K400.4}
which took on average $13.6$ and $23.3$ seconds to solve with and without the extra constraints
respectively.

Either way, we see no clear tendencies in these results. Sometimes adding the extra constraints
pay off in faster runtimes and sometimes it may cause slowdown. Maybe, with extra research, it
would be possible to determine whether certain characteristics in an instance make either option
more desirable.

It would also be interesting to see if disabling user cuts would make a difference with regards
to strengthening the model. As mentioned above, adding user cuts requires disabling part of
Gurobi's presolve. It is not unrealistic that the extra constraints could add something there.
\section{Conclusion}
In this chapter we have taken a look at the \acrlong{mtp}. We have defined the problem as
a graph optimisation problem on an undirected graph and then reformulated it as an \gls{ilp}
problem. As our main contribution, however, we have detailed and implemented a solver for the
problem based on the Gurobi optimisation suite on which we have performed a set of experiments
against a newly generated dataset.

Our new dataset, generated from a dataset for the \gls{pcstp}, comprises 22 instances both in
full and truncated forms (with 100 to 400 and 60 to 80 vertices respectively). This dataset
has been made available on ??? for further use. We have solved all truncated instances to
optimality, but leave the full instances to the reader.

For our solver, we have attempted to translate proven succesful methods for the \gls{pcstp}
into methods for
the \gls{mtp}.
In our experiments, we validated the usefulness of our primal heuristic callback and
a strengthened model to the point where they did not notably decrease performance.
We failed to show the same for our user cut callback, which significantly slowed down the
optimiser.

Based on these results, we are confident that it is possible to directly apply research on
the \gls{pcstp} to the \gls{mtp}. While there are some cases where additonal work must be
done to translate an algorithm ---e.g. the primal heuristic for which we skipped a step---
it seems that the problems are either generic enough or similar enough for information share.

For further work, it would be interesting to implement some of the refinements to the solver
mentioned above and see if the results of our experiments would change. Furthermore, we have
only tested the solver on a generic dataset. It would be interesting to idenfify and describe
real world instances of the \gls{mtp} and run the solver on these.


%%% Local Variables:
%%% TeX-master: "report"
%%% reftex-default-bibliography: ("lit.bib")
%%% End:
