\chapter{The Median Tree Problem}\label{chap:mediantree}
Having surveyed the state of research on the \gls{pcstp} closely along with a
short summary of related problems, we now turn our eyes to the lattermost problem
we have inspected: the \gls{mtp}.

As we noted in Chapter~\ref{chap:related},
the problem of finding median trees
in graphs is not very well researched. To the best of our knowledge, no work has been put into solving
the problem in the general case to optimality.
Being that this problem has some striking similarities
to the \gls{pcstp} ---foremost that both problems involve finding a connected
subgraph, which minimises the edge cost of the subgraph summed with
penalties paid for every excluded vertex--- we suspect that refitting the
methods used to solve the \gls{pcstp} to the \gls{mtp} may result in good performance.

In this chapter, we present a formal definition of the \acrlong{mtp} in Graphs
followed by a new \gls{ilp} formulation for the problem.
Then, we present a branch and cut algorithm for the \gls{mtp} which is inspired by the
work done on the \gls{pcstp}. Finally, we implement this algorithm as a solver with which
we perform a series of experiments on a newly generated dataset
for the \gls{mtp}, generated from a subset
of the DIMACS Steiner Tree challenge datasets for the \gls{pcstp} \citep{DIMACS}.
 
\section{Problem Definition}

Let $G = (V, E, c, d)$ be an undirected graph. Denote $c : E \to \RR^+$ as an \textit{edge cost} function
and $d : V \times V  \to \RR^+$ be an \textit{assignment cost} function where we have
\[d_{ii} = 0 \mathnormal{.}\]
Then the \textit{\acrlong{mtp}}
is defined as finding a \textit{connected subgraph} $T = (V_T, E_T)$ of $G$
where $V_T \subseteq V$ and
$E_T \subseteq E$ which minimises the cost function,
\[c(T) = \sum_{ij \in E_T} c_{ij} + \sum_{i \in V} \min_{j \in V_T} d_{ij}\mathnormal{.}\]
We say that such a subgraph is a \textit{Median Tree} of $G$.

\begin{figure}[h]\centering
  \begin{subfigure}[b]{0.47\linewidth}\centering
    \begin{tikzpicture}[auto, node distance=1.5 cm]
      % Nodes
      \node[terminal] (a) {a};
      \node[terminal] (b) [right=of a] {b};
      \node[terminal] (c) [right=of b] {c};
      \node[terminal] (d) [above =of c] {d};
      \node[terminal] (e) [left=of d] {e};
      \node[terminal] (f) [left=of e] {f};
      \node[terminal] (g) [above right=0.75 and 1.3 of c] {g};
      % Edges
      \begin{scope}[every edge/.style={draw=black, thick}]
        \draw (a) edge node[below]{4} (b);
        \draw (b) edge node[near start]{5} (d);
        \draw (b) edge node[below]{8} (c);
        \draw (c) edge node{3} (d);
        \draw (c) edge node{2} (g);
        \draw (c) edge node[near start]{5} (e);
        \draw (d) edge node{6} (e);
        \draw (d) edge node{10} (g);
        \draw (e) edge node{1} (f);
      \end{scope}
    \end{tikzpicture}
    \caption{The graph $G$ with edge costs.}\label{fig:mtp:01:g}
  \end{subfigure}
  \begin{subfigure}[b]{0.47\linewidth}\centering
    \footnotesize
    \begin{tabular}{r||c|c|c|c|c|c|c}
 $d_{ij}$ & $a$ & $b$ & $c$ & $d$ & $e$ & $f$ & $g$ \\ \hline\hline
      $a$ &  0  &  5  &  8  &     &     &  6  &     \\ \hline
      $b$ &  2  &  0  &  2  &  2  &     &     &     \\ \hline
      $c$ &     &     &  0  & 10  &  4  &     &  1  \\ \hline
      $d$ &     &  3  &     &  0  &  6  &     &  2  \\ \hline
      $e$ &     &     &     &     &  0  &     &     \\ \hline
      $f$ &  2  &  8  &     &     &  5  &  0  &     \\ \hline
      $g$ &     &     &  5  &  5  &     &     &  0
    \end{tabular}
    \caption{The assignment cost function $d$.}\label{fig:mtp:01:d}
    \end{subfigure}
  \caption{Instance of the \gls{mtp} problem.}
  \label{fig:mtp:01}
\end{figure}

Figure~\ref{fig:mtp:01} shows an example of the \gls{mtp} represented as a graph
(Figure~\ref{fig:mtp:01:g}) taken from our recurring example (Figure~\ref{fig:pcstp:01})
and an assignment function (\ref{fig:mtp:01:d}) for which the values have been
chosen arbitrarily.
Figure~\ref{fig:mtp:01:opt}
shows the optimal solution
\[T = ( \{ c, e, f, g \}, \{(c, e), (c, g), (e, f)\})\]
to this problem with total cost
\[c(T) = (1 + 5 + 2) + (6 + 2 + 2) = 18\mathnormal{.}\]
\begin{figure}[h!]
  \centering
      \begin{tikzpicture}[auto, node distance=1.5 cm]
      % Nodes
      \node[terminal] (a) {a};
      \node[terminal] (b) [right=of a] {b};
      \node[terminal] (c) [right=of b] {c};
      \node[terminal] (d) [above =of c] {d};
      \node[terminal] (e) [left=of d] {e};
      \node[terminal] (f) [left=of e] {f};
      \node[terminal] (g) [above right=0.75 and 1.3 of c] {g};
      % Edges
      \begin{scope}[every edge/.style={draw=black, thick}]
        \draw (a) edge node[below]{4} (b);
        \draw (b) edge node[near start]{5} (d);
        \draw (b) edge node[below]{8} (c);
        \draw (c) edge node{3} (d);
        \draw (c) edge[selected] node{2} (g);
        \draw (c) edge[selected] node[near start]{5} (e);
        \draw (d) edge node{6} (e);
        \draw (d) edge node{10} (g);
        \draw (e) edge[selected] node{1} (f);
      \end{scope}
      \begin{scope}[every edge/.style={assignment}]
        \draw[->] (a) edge node{6} (f);
        \draw[->] (b) edge[bend right=60] node[below]{2} (c);
        \draw[->] (d) edge[bend left=60] node{2} (g);
      \end{scope}
    \end{tikzpicture}
    \caption{Optimal solution to the \gls{mtp} problem in Figure~(\ref{fig:mtp:01}).
      The edges of the facility are coloured in red and assignments are denoted with translucent red arrows.}\label{fig:mtp:01:opt}
  \end{figure}

  Since there exists a relative balance between the cost of edges in the graph and the assignment costs, $T$
  only spans part of the graph. This raises a point. As assignment costs tend to infinity, the \gls{mtp} begins to look
  like the Minimum Spanning Tree problem.
  This is akin to the \gls{stp} with $N = V$ and the \gls{pcstp} with infinite prize
  on all vertices. However, when edge costs tend toward infinity, the facility will naturally become a single vertex
  and the \gls{mtp} will begin to look like the $1$-Median problem (See Section~\ref{sec:related:median}).

\paragraph{Reduction from the Prize-Collecting Steiner Tree Problem}
Instances of the \gls{pcstp} can straightforwardly be reduced to instances of the \gls{mtp}. This is in exact correspondence
with how instances
of the \gls{ptp} can be reduced to instances of the Median Tour Problem (Section~\ref{sec:related:median}).
Given an instance of the \gls{pcstp} on graph $G = (V, E, c, p)$, define the assignment cost function

$$d_{ij} =
 \begin{cases}
   0 & i = j \\
   p_i & i \neq j
 \end{cases}\mathnormal{.}
 $$
 then solving the \gls{mtp} on the graph $G' = (V, E, c, d)$ is equivalent to solving the \gls{pcstp} on $G$. Since every
 vertex which is not assigned to itself will incur a penalty which costs the \textit{prize} of that vertex,
 every vertex not in the facility will pay its prize as penalty. Hence,
 any solution $T = (V, E, c, p)$ to the \gls{pcstp} on $G$ will have the exact
 same cost as the solution $T' = (V, E, c, d)$ to the \gls{mtp} on $G'$.

\begin{figure}[h]\centering
  \begin{subfigure}[b]{0.47\linewidth}\centering
    \begin{tikzpicture}[auto, node distance=1.5 cm]
      % Nodes
      \node[terminal] (a) {a};
      \node[terminal] (b) [right=of a] {b};
      \node[terminal] (c) [right=of b] {c};
      \node[terminal] (d) [above =of c] {d};
      \node[terminal] (e) [left=of d] {e};
      \node[terminal] (f) [left=of e] {f};
      \node[terminal] (g) [above right=0.75 and 1.3 of c] {g};
      % Edges
      \begin{scope}[every edge/.style={draw=black, thick}]
        \draw (a) edge node[below]{4} (b);
        \draw (b) edge node[near start]{5} (d);
        \draw (b) edge node[below]{8} (c);
        \draw (c) edge node{3} (d);
        \draw (c) edge node{2} (g);
        \draw (c) edge node[near start]{5} (e);
        \draw (d) edge node{6} (e);
        \draw (d) edge node{10} (g);
        \draw (e) edge node{1} (f);
      \end{scope}
    \end{tikzpicture}
    \caption{The graph $G$ with edge costs.}
    \label{fig:mtp:pcstp:g}
  \end{subfigure}
  \begin{subfigure}[b]{0.47\linewidth}\centering
    \footnotesize
    \begin{tabular}{r||c|c|c|c|c|c|c}
 $d_{ij}$ & $a$ & $b$ & $c$ & $d$ & $e$ & $f$ & $g$ \\ \hline\hline
      $a$ &  0  &  12 &  12 &  12 &  12 &  12 &  12 \\ \hline
      $b$ &  0  &  0  &  0  &  0  &  0  &  0  &  0  \\ \hline
      $c$ &  0  &  0  &  0  & 0  &  0  &  0  &  0  \\ \hline
      $d$ &  10 &  10 &  10 &  0  &  10 &  10 &  10 \\ \hline
      $e$ &  0  &  0  &  0  &  0  &  0  &  0  &  0  \\ \hline
      $f$ &  0  &   0 &  0  &  0  &  0  &  0  &  0  \\ \hline
      $g$ &  3  &   3 &  3  &  3  &  3  &  3  &  0
    \end{tabular}
    \caption{The assignment cost function $d$.}
  \end{subfigure}

  \begin{subfigure}[b]{0.60\linewidth}\centering
    \begin{tikzpicture}[auto, node distance=1.5 cm]
      % Nodes
      \node[terminal] (a) {a};
      \node[terminal] (b) [right=of a] {b};
      \node[terminal] (c) [right=of b] {c};
      \node[terminal] (d) [above =of c] {d};
      \node[terminal] (e) [left=of d] {e};
      \node[terminal] (f) [left=of e] {f};
      \node[terminal] (g) [above right=0.75 and 1.3 of c] {g};
      % Edges
      \begin{scope}[every edge/.style={draw=black, thick}]
        \draw (a) edge[selected] node[below]{4} (b);
        \draw (b) edge[selected] node[near start]{5} (d);
        \draw (b) edge node[below]{8} (c);
        \draw (c) edge node{3} (d);
        \draw (c) edge node{2} (g);
        \draw (c) edge node[near start]{5} (e);
        \draw (d) edge node{6} (e);
        \draw (d) edge node{10} (g);
        \draw (e) edge node{1} (f);
      \end{scope}
      \begin{scope}[every edge/.style={assignment}]
        \draw[->] (c) edge[bend left=60] node{0} (b);
        \draw[->] (e) edge node[left]{0} (b);
        \draw[->] (f) edge node{0} (a);
        \draw[->] (g) edge[bend right=60] node[above]{3} (d);
      \end{scope}
    \end{tikzpicture}
    \caption{Optimal Solution to (\ref{fig:mtp:pcstp:g}).}
  \end{subfigure}

  \caption{Instance of the \gls{pcstp} (Figure~\ref{fig:pcstp:01}) with optimal solution.}
  \label{fig:mtp:pcstp}
\end{figure}

Figure~\ref{fig:mtp:pcstp} shows how this reduction works on the \gls{pcstp} instance in Figure~\ref{fig:pcstp:01}
as well as the optimal solution
\[T = (\{a,b,d\}, \{(a,b), (b,d)\})\]
with cost
\[c(T) = (4 + 5) + (3) = 12\]
which is the same cost as the optimal solution to the original \gls{pcstp} problem (Figure~\ref{fig:pcstp:01:opt}).

While it is obvious that the \gls{mtp} problem is NP-hard
---something which has already been established by previous literature
(See Section~\ref{sec:related:median})---
this reduction again implies the NP-hardness of the problem.
 \section{Applications}
 We expect the \acrlong{mtp} to have similar applications as the \gls{pcstp} as they are fairly similar problems.

 Foremost, any kind of problem which involves a two tiered supply system could possibly be modelled as an instance of the
 \gls{mtp}. Take, for example, the placement of postal warehouses.
 Building and staffing a new warehouse comes at
 a cost but makes it easier to deliver packages to the surrounding area.
 The cost of warehouses (and the cost of maintaining
 a transport networks between them) could be modelled as edge costs, while the cost of delivering items from a specific warehouse to an area
 could be modelled as assignment costs.  Similar scenarios could exist in water supply, military supply lines, aid distribution etc.

 Another area of interest may be within computation biology, which is an area where the \gls{pcstp} is put to use
 \citep{akhmedov2016divide, sun2018classical}.
\section{Integer Programming Formulation}

Given an instance of the \gls{mtp} as defined above,
Formulation~(\ref{form:mtp:cut}) is an \gls{ilp} formulation of the \gls{mtp}.
It is loosely inspired by the one defined by \citet{lucena2004strong}
(Formulation~\ref{form:lower:gsec}, Section~\ref{sec:lower:gsec}) for the
\gls{pcstp}.

The variables $\bd x$ and $\bd y$ are boolean
decision vectors which are interpreted as follows.
As with the \gls{pcstp} formulation, when $x_{ij} = 1$,
the edge between vertices $v_i$ and $v_j$ is part of the solution.
Note that only
one of the variables $x_{ij}$ or $x_{ji}$ are variables
in the model and we use both indices interchangeably
to refer to the same variable for the sake of readability.

Similarly, $\bd y$ describes the assignment relation of a solution. When
$y_{ij} = 1$, vertex $v_i$ is \textit{assigned} to vertex $v_j$ and the corresponding
assignment cost must be paid.
These relations are reflected in the objective function.

When $y_{kk} = 1$, we consider that vertex assigned to
itself, which implies that vertex $v_k$ is part of the facility.

 \begin{formulation}[h!]
   \begin{subequations}
     \begin{alignat}{3}
       &\underset{\bd x, \bd y}{\text{minimize}}
       & & \sum_{ij \in E} c_{ij} x_{ij} +  \sum_{i, j \in V} d_{ij}y_{ij}  & \\
       & \text{subject to}\quad
       & & \sum_{ij \in E} x_{ij} = \sum_{i \in V} y_{ii} - 1 &&  \label{form:mtp:tree}\\
       &&& x(E(S)) \leq \sum_{i \in S \setminus \{s\}} y_{ii}
       && \forall S \subseteq V, s \in S \label{form:mtp:gsec} \\
       &&& \sum_{j \in V} y_{kj} = 1 && \forall k \in V \label{form:mtp:assignment}\\
       &&& y_{ik} \leq  y_{kk}
       && \forall i, k \in V \label{form:mtp:facility}\\
       &&& y_{kk} \leq \sum_{i \in \gls{delta}(k)} x_{ik}
       && \forall k \in V \label{form:mtp:legal} \\
       &&& \bd x \in \BB^{|E|} && \\
       &&& \bd y \in \BB^{|V \times V|}
     \end{alignat}\label{form:mtp:cut}
   \end{subequations}
   \caption{\gls{ilp} formulation of the \gls{mtp}.}
 \end{formulation}

 Since edges have non-negative weights, we know that the shape of the facility must be a tree.
 Constraint (\ref{form:mtp:tree}) ensures that any facility has exactly one less edge than
 vertices, and constraints (\ref{form:mtp:gsec})
 ---commonly known as \glspl{gsec}---
 ensure that no subset of vertices in the facility can form
 a cycle. Together, these ensure that any feasible
 solution must be a single, connected subgraph of $G$
 which must be shaped as a tree.

 The rest of the constraints are concerned with making sure that for every feasible solution
 each vertex in $G$ must assigned to some vertex which is part of the facility.
 Constraint
 (\ref{form:mtp:assignment}) ensures that every vertex in $G$ is assigned to exactly one
 vertex, and constraint (\ref{form:mtp:facility}) ensures that a vertex can only be the target
 of an assignment if it is part of the facility. The latter could alternatively be modelled with a set
 of \textit{Big M} constraints as
 \[\sum_{i \in V} y_{ik} \leq  M y_{kk}  \qquad \forall i \in V\]
 for some $M \gg |V|$. This ensures that if $v_k$ is part of the facility then
 \textit{any number} of
 vertices (or $M$ vertices, to be precise) can be assigned to it, but if not
 then $0$ vertices can be assigned to it.

 Finally, the constraints (\ref{form:mtp:legal}) ensure that no vertex can be part
 of the facility unless an edge in its adjacency is selected in any feasible solution.
 In other words, the facility is constrained to being
 the set of vertices spanned by the solution tree.

 \paragraph{Valid Inequalities}
 Whenever a vertex is connected to the facility, it is trivially correct to assign it to itself.
 This follows directly from the fact that assignment costs are non-negative.
 So, since we have $d_{ii} = 0$
 for all $i \in V$
 we also must have $d_{ij} \geq d_{ii}$
 for all $i,j \in V$.
 In our model, this fact is more-or-less implied by the objective function.
 
 However, the model does not disallow zero cost assignments between
 vertices, and as such we \textit{could} end with a solution where we have $y_{ii} = 0$ for some
 vertex $i$ even though $i$ is in fact spanned by the tree. There would then exist an equivalent
 solution where vertex $i$'s assignment is switched to itself. Both of these solutions would translate
 to the same graph.

 We can formalise this, and allow only \textit{canonical} solutions where all vertices
 which are spanned by the edges of the facility are also assigned to themselves by
 adding the constraints,
 \begin{equation}\label{form:mtp:str}
 y_{ii} \geq x_{ji} \qquad \forall i \in V,  \forall j \in \delta(i)\mathnormal{.}
\end{equation}
This increases the size of the model by up to $|V|^2$ constraints, but
decreases the amount of feasible solutions and potentially the amount of optimal solutions.
\medskip

We can find another set of valid constraints by looking at how \citet{ljubic2005solving}
constrain the degree of nonterminals in
Constraint~(\ref{form:exact:strength}) in their model of the \gls{pcsap}.
We can apply the same idea to the \gls{mtp}, but in a slightly different manner.

 Consider the set of vertices which cost nothing to assign and cannot be assigned to as \textit{Nonterminals},

 \[N = \left\{ i \in V \middle\vert \forall j.\: d_{ij} = 0 \wedge   d_{ji} = \infty\right\}\mathnormal{.}\]

 As nonterminals play no part in the assignment question,
 they cannot be a leaf node in an optimal solution.
 Any leaf-node nonterminal could trivially
 be removed from the facility to produce a solution with no greater cost.
 Thus they must have degree of at least
 \textit{two} should they be part of an optimal facility.

 Hence, the constraint
 \begin{equation}\label{mtp:valid:deg}
   \sum_{j \in \delta(i)}x_{ij} \geq 2 x_{ik} \qquad \forall i \in N, \: \forall k \in \delta(i)
 \end{equation}
 is valid for our model to the model.

 The intuitive interpretation of (\ref{mtp:valid:deg}) is that if any edge adjacent to
 a nonterminal is selected, \textit{at least} two edges adjacent to the nonterminal must be selected.
\section{Solver}\label{sec:mtp:solver}
Based in part on results from our survey of the \gls{pcstp} in Chapter~\ref{chap:solving},
we present here
a solver for the \gls{mtp}. This section both goes over algorithmic and technical
implementation details of the solver

The \gls{mtp} solver makes use of the general purpose \gls{mip} solver
Gurobi \citep{gurobi} (version 8.0.1) as an \gls{ilp} framework
and its Python 3 interface for implementing and applying user callbacks.
Using Python as a callback interface obviously comes at a performance cost.
Had we chosen to make use of Gurobi's C or C++ interfaces,
we would undoubtedly see better performance.
However, the Python interface was chosen for its higher level interface
and better ease-of-use which has resulted in shorter iteration times.
Having no other computational results
to compare against, this trade-off was easy to make.

The Python library,
NetworkX\footnote{\url{https://networkx.github.io/}} \citep{hagberg2008exploring}
was used for the internal graph representation of the
problem instances in the solver as well as for its implementation of common
graph algorithms.
It is worth noting that NetworkX features algorithm implementations
in native Python instead of the more efficient Python C modules. This, again, has some
performance implications especially with regards to user callbacks (which we
 touch on below).

The source code of the solver as well as the benchmark suite
can be found on GitHub\footnote{\url{https://github.com/wsprent/thesis-code}}.

\paragraph{Preprocessing}

While we have seen that preprocessing for the \gls{pcstp} is a \textit{very} important
tool in solving hard instances, our solver does not include any preprocessing routines.

This is not a design choice but rather a limitation. The \gls{mtp} is
more combinatorially
complex than the \gls{pcstp} and this makes the preprocessing routines defined for
the \gls{pcstp} incompatible with the \gls{mtp}.
For example, while removing an edge or a vertex in the \gls{pcstp}
has very straightforward consequences in
specific cases, it may have cascading effects in same cases for the \gls{mtp}.

We have simply yet to uncover which preprocessing routines can be applied to instances of
the \gls{mtp}.

\paragraph{Branch and Cut}
The choice of applying a branch and cut method in the solver, is fairly straight
forward to make as it is infeasible to load all of the exponential number of
\glspl{gsec} into the model initially. Thus, the solver
\textit{must} make use of
Gurobi's \textit{Lazy Constraint} callbacks to avoid producing infeasible solutions.
Additionally, we \textit{may} apply \glspl{gsec} as
\textit{User Cuts} to LP relaxation solutions to potentially
give faster runtimes by producing tighter lower bounds.

As we are going to be separating \glspl{gsec} anyway, we might as well generate cuts
for the LP solution. Hence, we include both lazy constraints and user cuts
in our solver through Gurobi's callback system.
\subparagraph{Lazy Constraints}
The job of the lazy constraint callback is to separate integer solutions
which violate any \glspl{gsec}. This boils down to finding cycles in the solution
graph. If this is not done, the optimiser may incorrectly return infeasible solutions.

Given an integer solution $(\bd{\hat{x}}, \bd{\hat{y}})$ produced by Gurobi, we
identify any violated \glspl{gsec} by first generating a NetworkX graph,
\[S = (\{i \in V \mid \hat y_{ii} = 1 \}, \{ (i, j) \in E \mid \hat x_{ij} = 1\})\mathnormal{.}\]
Then we again use NetworkX to find all simple cycles in $S$, and for all vertex
subsets $C \subseteq V$ induced by a cycle
in $S$, we add all of the violated constraints
\[x(E(C)) \leq y_{ii} \setminus \{c\} \qquad \forall c \in C\]
to the model.
\subparagraph{User Cuts}
For user cuts, we have implemented a routine which is
responsible for separating violated \glspl{gsec} in the LP relaxation solution in every node of
the branch and bound tree.
This routine has been adapted from the max-flow/min-cut method presented by \citet{lucena2004strong}
 (see Section~\ref{sec:solving:lower}) for the \gls{pcstp}.
 This is also the method used for the \gls{pcstp} solvers presented by
 \citet{ljubic2005solving} and \citet{gamrath2017scip}.

Adapting this separation procedure can be done directly. Having the decision variable
$\hat y_{ii} = b$ for some $b \in [0,1]$ in the LP relaxation of
the \gls{mtp} corresponds directly to $\hat y_i = b$ in the \gls{pcstp}.
Once again, we use NetworkX to both represent the support graph and solve max flow problem(s).

As with the DHEA solver by \citet{ljubic2005solving}, we allow for controlling the maximum amount
of cuts performed at each node in the branch and bound tree
with a \texttt{-{}-max-cuts} option. Additionally, this can be set
to \texttt{0} to disable the user cut routine altogether.

\paragraph{Primal Heuristics}
The solver includes an optional custom primal heuristic adapted from the primal heuristics detailed
for the \gls{pcstp} by \citet{ljubic2005solving}
(see Sections~\ref{sec:solving:exact}~and,~\ref{sec:heuristics:lp}).

As with the cuts above, we exploit the fact that $y_i$ in the \gls{pcstp}
corresponds directly to $y_{ii}$ in the \gls{mtp}.

One thing we have not adapted, however, is the last step of solving the \gls{mtp}
on the final tree. This is left for further research.

The primal heuristic can be disabled in favour of Gurobi's inbuilt heuristic with
the switch \[\texttt{-{}-no-heuristics}\mathnormal{.}\]
\section{Computational Experience}
With having no prior attempts to solve the \gls{mtp} in the general case to draw from,
we have no interesting timings to test our model against nor any previously constructed
datasets to perform benchmarks on. Thus, we present in this section two things,
\begin{enumerate}
\item The specifics of how we have generated a new dataset for the \gls{mtp} out of previously released
  datasets for the \gls{pcstp},
  as well as the details of the dataset, and
\item The result of performing benchmarks which measure the effectiveness of
  \begin{enumerate*}[label={\alph*)}, font=\bfseries]
  \item our primal heuristic,
  \item the user cut routine, and
  \item the extra constraint (\ref{form:mtp:str}).
  \end{enumerate*}
\end{enumerate}
In short, we are looking to test out our additions to what could be called a ``default'' Gurobi
implementation of the \gls{mtp}.

First, we give an overview of the dataset. This is then
followed by an inspection of the three experiments we have
 performed.
 \subsection{Datasets}
 To perform realistic and relevant benchmarks of the settings in our solver, we need to run the solver
 against instances of the \gls{mtp} which are large enough to not be presolved and preferably small
 enough to solve to optimality within a reasonable time frame. At the same time, as this is an introduction
 to the problem, we want the instances to be generic, opposed to modelling some specific application or
 edge case of the \gls{mtp}.

 In getting to this point, our starting point is the \texttt{JMP} dataset presented by 
 \citet{Johnson:2000:PCS:338219.338637}. This was picked mainly for featuring the smallest
 and most easy to solve instances among the available \gls{pcstp} datasets.
 And yet ---as seen below--- we have had to truncate the instances to make them solvable
 in reasonable amounts of time.

 The input instances are distributed in the \texttt{STP} format used by the \textit{SteinLib}
 library \citep{koch2001steinlib}, and since we can easily extend the \texttt{STP} format
 to describe the \gls{mtp}, we describe the resulting instances in this format as well.

 Extending the \texttt{STP} involves the addition of a new
 \texttt{Section} named \texttt{AssignmentCosts}. Each line
 in this section describes the cost of assigning a single vertex to another.

 Formally, each line has the format,
 \begin{grammar}
   <assignment> := `D' `\ ' <vertex-id> `\ ' <vertex-id> `\ ' <cost>
 \end{grammar}
 where \textit{<vertex-id>} is an integer id which corresponds to a vertex declared in the \texttt{Graph}
 section of the same file, and \textit{<cost>} is a non-negative real number.

 For example, the following example reads as \textit{the cost of assigning vertex ``1'' to vertex ``2'' is $4.5$.}
\begin{verbatim}
D 1 2 4.5
\end{verbatim}
 
 For reference, Appendix~\ref{app:report:stp} shows Figure~\ref{fig:mtp:01}
 represented in the extended STP format.

 \paragraph{Translation}
 In translating the \gls{pcstp} instances into \gls{mtp} instances, we keep all the of the graph
 structure including the edge costs. Hence we only need to generate a set of assignment costs which
 are \textit{balanced} with respect to edge costs.

 To generate assignment costs, we first assign any nonterminal with no prize a prize
 equal to the average prize across all vertices. The cost of assigning some vertex $i$
 to some vertex $j$ is then,
 $$d_{ij} =
 \begin{cases}
   p_i \cdot \frac{sp(i, j)}{\sum_{l \in V}sp(i,l) / |V|} & p_i > 0 \\
      \frac{\sum_{l \in V} p_l}{|V|} \cdot \frac{sp(i, j)}{\sum_{l \in V}sp(i,l) / |V|} & p_i = 0
 \end{cases}\mathnormal{.}
$$
In other words, vertex $i$'s prize is factored by the ratio between the
length of the shortest path
between $i$ and $j$ and the
average distance between vertex $i$ and all vertices.
This keeps the prizes in the original dataset relevant, and makes ``closeness'' between
vertices with respect to assignment costs somewhat transitive.

 A facet --- perhaps a weakness --- of this method is that it produces somewhat uniform
 assignment costs, and that it requires a connected graph to produce an \gls{mtp} instance
 with a lower-than-infinity optimal value. Furthermore, on connected graphs, all vertices
 are assignable to all vertices, and no vertex is assignable to another vertex at no cost.

 Some of the instances in the \texttt{JMP} dataset are disconnected. For these instances,
 before performing the above, we connect the graph by selecting a random pair of vertices,
 $i,j$, from disjoint components. The edge $(i,j)$ is then added to the edge set with cost
 \[c_{ij} = \sum_{k \in \delta(i)} c_{ik} / |\delta(i)|\mathnormal{.}\]

 To summarise, the output dataset features instances of the \gls{mtp}
 on connected graphs with
 \textit{smooth}
 ---as opposed to erratic---
 assignment cost functions.
 All vertices
 can be assigned to all other vertices without incurring infinite cost,
 and no vertex
 can be assigned to another vertex at zero cost. 

 \paragraph{Truncation} However, even though we chose the ``easiest'' of the available
 \gls{pcstp} datasets,
 our solver still struggle with our translated instances, and would require a timeout
 value too large for our time constraints.

 Thus we have chosen to create smaller instances by truncating the translated \texttt{JMP}
 instances. The translated dataset mainly contains instances with either $100$ or $400$ vertices,
 and we truncate these into instances containing $60$ and $80$ vertices respectively.

 We accomplish this by iteratively contracting randomly selected edges. After contracting edges,
 we then randomly remove edges from the graph until we have the same node / edge ratio as in the
 full graph.

 This method is admittedly rather arbitrary, but it aims to accomplish a graph containing similar
 connectivity as well as introducing a level of randomness to ---hopefully--- avoid a too uniform graph.

 The end result from the above is two sets of instances, \texttt{JMP-60} and \texttt{JMP-80}, named
 after the original dataset and the amount of vertices they contain.

\paragraph{The Dataset}
Table~\ref{tab:mtp:dataset} shows an overview of the truncated dataset which is the basis for our
experiments. A similar table for the full instances can be seen in Table~\ref{tab:app:jmp}.

\begin{table}[h!]
  \centering
  \begin{subtable}{0.45\linewidth}
    \centering
    \begin{tabular}[h!]{c|c|c}
      \input{dataset-jmp60-table.tex}
    \end{tabular}
    \caption{JMP-60.}
  \end{subtable}
  \begin{subtable}{0.45\linewidth}
    \centering
  \begin{tabular}[h!]{c|c|c}
    \input{dataset-jmp80-table.tex}
  \end{tabular}
  \caption{JMP-80.}
  \end{subtable}
  \caption{The \gls{mtp} dataset.}\label{tab:mtp:dataset}
\end{table}

While especially the JMP80 instances are slightly more sparse than expected, we have mostly
achieved what we wanted with the instance sizes. Moreover, as seen below they behave well
with respect to the critea we set forth above.
 \subsection{Experiments}

 To perform our three experiments, we have developed a small rudimentary benchmarking suite. The solver
 itself is equipped with the ability to time itself and perform repeated solving of the same problem.
 For each problem instance, the solver is run with a \texttt{-{}-timeout} of five minutes. This timeout
 is passed directly to Gurobi, so the actual timeout is five minutes plus the time it takes to load the
 graph etc. Each instance is solved five times (the problem is loaded from file each of the five times)
 and the all times reported below is the mean these five timings.

 Executions are grouped by instance, that is for each instance, the solver is run with the following
 settings in order:
 \begin{enumerate}
 \item No user cuts (\texttt{-{}-max-cuts 0 -{}-no-heuristics}).
 \item Maximum of one cut per node (\texttt{-{}-max-cuts 1 -{}-no-heuristics}).
 \item Maximum of 25 cuts per node (\texttt{-{}-max-cuts 25 -{}-no-heuristics}).
 \item Primal heuristic (\texttt{-{}-max-cuts 0}).
 \item With constraint~(\ref{mtp:valid:deg}) (\texttt{-{}-max-cuts 0 -{}-strengthen -{}-no-heuristics}).
 \end{enumerate}
 Grouping benchmarks by instances was done to hopefully improve the chances of each run happening
 under the same circumstances while the precise ordering of the settings is arbitrary.

 We have chosen our ``default'' settings as close to default Gurobi as possible. Thus, when we test
 a feature we have implemented, we do so isolation.
 
 All experiments were run on a machine with a Intel Core i7 CPU with a $2.8$ GHz clockrate, 4
 cores, and 8 hardware threads along with $16$GB DDR3 RAM.

\paragraph{Heuristics}
In this experiment, we look at the performance of our primal heuristic, defined above, versus
the inbuilt heuristic from Gurobi. While the Gurobi heuristic, which is based on rounding
of the the LP solution, has the advantages of being written in a systems programming language
with less overhead and ---probably--- has hooks directly into the core optimiser,
implementing a strong heuristic for the \gls{mtp} should be able to out perform it.

In the case of this particular formulation of the \gls{mtp},
for example, the Gurobi optimiser does not have access to the
many \glspl{gsec}, making it prone to produce infeasible solutions which must immediately
be separated by the lazy constraint callback.

At the same time, our heuristic is adapted directly from a neighbouring problem,
and implemented in an interpreted language. It may struggle to keep up.

For our benchmarks, we have kept the default Gurobi setting of approximately running
the default heuristic $5\%$ of the time when enabled. Our heuristic instead runs on
every 25th node.
\bigskip

\begin{table}[h!]
  \centering
  \begin{tabular}[h!]{|c|c|c|c|}\hline
    \input{heuristics-table.tex}
  \end{tabular}
  \caption{Timing results for the primal heuristic benchmark. The
    \textbf{bolded} times are the fastest.}
  \label{tab:jmp:heuristics}
\end{table}

In Table~\ref{tab:jmp:heuristics} we show the timings of running the heuristics experiment.
In both cases, all instances were solved to optimality within the five minute time limit.
The results are split down the middle, with both configurations being the fastest
in 11 out of 22 instances. Some instances see a very small discrepancy between the fastest
and the slowest timing such as \texttt{K400.7} which has less than a seconds difference,
while others see a more significant gap.


This mirrors the results from
\citet{ljubic2005solving}. They report that their primal heuristic (which we have based
ours on) turns out to improve runtimes in only some instances against the default heuristic
in the CPLEX optimiser.
\bigskip

While our results are somewhat ambivalent,
they also suggest that perhaps by improving the heuristic we can achieve performance
which eclipses the default performance.

How we should achieve this, depends heavily on which way the heuristic is ``underperforming''.
 Is it producing solutions of too high value or is it running too slowly?

 If speed is a problem, a good first option to improve performance,
 would be to rewrite the heuristic procedure
in a systems programming language (e.g. C, C++, Rust).
This should by default give a multi-factor speedup, and gives better options for
later performance optimisation.

If the heuristic isn't producing good enough solutions, then we could look at the final step
of the algorithm. In our implementation we do not solve the \gls{mtp} on the final tree contrary
to the original algorithm defined by \citet{ljubic2005solving}. It may be that by skipping
this step, we are missing out on a lot.

In both cases, simply running the heuristic on every 25th node is not a very cunning
way of using our resources, and there is no reason to believe that this achieves optimal
performance.
If the heuristic was re-implemented to be blazingly fast, it may be advisable to
simply run the full heuristic at every node of the branch and bound tree
(which mirrors what we have seen in Section~\ref{sec:solving:exact}). However, it may also
be a good idea to change behaviour based on the nature of the LP solution, either by
running a different heuristic when the LP solution far from integral or \textit{only}
running a heuristic when the solution is close to integral.

Either way, our results seem to confirm that reusing the heuristic from
\citeauthor{ljubic2005solving} is not a faulty strategy, and that the \gls{pcstp}
and \gls{mtp} similar enough for knowledge sharing in this case.

\paragraph{User Cuts}
A very different story is told by our user cuts experiment. Table~\ref{tab:jmp:maxcut}
displays the results of our user cut experiment. The results are very clear, the performance
of our user cut routine is atrocious compared to disabling it altogether. In all 22 instances
the MC-25 option falls for the five minute timeout and fails to solve the instance to optimality.
The same happens for the MC-1 option in two cases.

Moreover, we more often than not see more than a 2x slowdown from the MC-0 option to the
MC-1 option. In one instance, we do see that the MC-1 option outperforms the MC-0.
However, with
timings around the two second mark this is probably not very significant, and could
perhaps be explained by a hike in the presolve or similar.
\medskip

In the end, there is a very stark difference between the performance
 of the three configurations.
This raises the question of whether or not we are talking about a performance
related bug in the implementation of the user cut routine. This does not lead to
interesting discussion, though, so we will proceed as if this is not the cause.

\begin{table}[h!]
  \centering
  \begin{tabular}[h!]{|c|c|c|c|c|}\hline
    \input{max-cut-table.tex}
  \end{tabular}
  \caption{Timing results for running the user cut experiments. The columns MC-0,
    MC-1, and MC-25 shown the timings for running the solver with no user cuts, max
    one user cut per node, and max 25 user cuts per node respectively.
    \textbf{Bold} font indicates fastest time. Instances which fell for the five minute
  timeout ($*$) have their LP gap displayed in place of time.}\label{tab:jmp:maxcut}
\end{table}

Another possible cause is that we are spending too much time in the slower Python
interface. Having to potentially
solve a max-flow problem for each vertex in the input graph incurs a runtime
of $O(n^2 \log n)$. This makes the separation procedure quite costly compared
to our primal heuristic.
Spending this much time in the Python interpreter may actually be enough to explain
even these performance
discrepancies.

Furthermore, enabling user cuts in Gurobi requires disabling the \texttt{PreCrush} setting
which disallows Gurobi from reformulating constraints when presolving the model. This means
that a user cut routine must not just be able to justify
itself in terms of the extra time spent for the tighter bounds,
but also with respect to a weakening of the presolving routine.

In the end, these results show that disabling user cuts altogether in our model is definitely
the optimal option, and drastic improvements to the routine need to happen before this may
change.

\paragraph{Strengthening the Model}
Our final experiment involves modifying the model. Thus, we only indirectly affect the
behaviour of the Gurobi solver.
This obviously involves a trade off between increasing the size of the model and reducing the size
of the feasible region, which may or may not give positive results.

\begin{table}[h!]
  \centering
  \begin{tabular}[h!]{|c|c|c|c|}\hline
    \input{strengthen-table.tex}
  \end{tabular}
  \caption{Timing results with and without constraint constraints~(\ref{form:mtp:str}).
    \textbf{Bold} font indicates fastest time; Instances which fell for the five minute
  timeout ($*$) have their LP gap displayed in place of time.} 
  \label{tab:jmp:strengthen}
\end{table}

Table~\ref{tab:jmp:strengthen} shows the effects of adding constraints~(\ref{form:mtp:str}) to
the model. For most of the instances there is very little difference between the two models
of the problem in how long it takes to reach an optimal solution. There are some exceptions, though.
For example, instance \texttt{K100} is solved in about $12$ seconds with the original mode and
$26$ seconds with the strengthened model.

Either way, we see no clear tendencies in these results. Sometimes adding the extra constraints
pay off in faster runtimes and sometimes it may cause slowdown. Maybe, with extra research, it
would be possible to determine whether certain characteristics in an instance make either option
more desirable.

\section{Conclusion}
In this chapter, we have taken a look at the \acrlong{mtp}. We have defined the problem as
a graph optimisation problem on an undirected graph and then reformulated it as an \gls{ilp}
problem. As our main contribution, however, we have detailed and implemented a solver for the
problem based on the Gurobi optimisation suite on which we have performed a set of experiments
against a newly generated dataset.

Our new dataset, generated from a dataset for the \gls{pcstp}, comprises of 34 full instances
and 22 truncated instance. This dataset
has been made available on
GitHub\footnote{\url{https://github.com/wsprent/thesis-instances}}
for further use. We have solved all truncated instances to
optimality, but leave the full instances to the reader.

For our solver, we have attempted to translate proven successful methods for the \gls{pcstp}
into methods for
the \gls{mtp}.
In our experiments, we validated the usefulness of our primal heuristic callback and
a strengthened model to the point where they at least did not
decrease performance.
We failed to show the same for our user cut callback, which significantly slowed down the
optimiser.

Based on these results, we are confident that it is possible to directly apply research for
the \gls{pcstp} to the \gls{mtp}. While there are some cases where additional work must be
done to translate an algorithm ---e.g. the primal heuristic for which we skipped a step---
it seems that the problems are either generic enough or similar enough for information sharing
to be a succesful endeavour.

For further work, it would be interesting to implement some of the refinements to the solver
mentioned above and see if the results of our experiments would change. Furthermore, we have
only tested the solver on a generic dataset. It would be interesting to identify and describe
real world instances of the \gls{mtp} and run the solver on these.


%%% Local Variables:
%%% TeX-master: "report"
%%% reftex-default-bibliography: ("lit.bib")
%%% End:
